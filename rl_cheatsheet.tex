\documentclass[10pt,landscape]{article}
\usepackage{multicol}
\usepackage{calc}
\usepackage{ifthen}
\usepackage[landscape]{geometry}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage{algorithm2e}
\usepackage{float}

% To make this come out properly in landscape mode, do one of the following
% 1.
%  pdflatex rl_cheatsheet.tex
%
% 2.
%  latex rl_cheatsheet.tex
%  dvips -P pdf  -t landscape rl_cheatsheet.dvi
%  ps2pdf rl_cheatsheet.ps


% If you're reading this, be prepared for confusion.  Making this was
% a learning experience for me, and it shows.  Much of the placement
% was hacked in; if you make it better, let me know...


% 2008-04
% Changed page margin code to use the geometry package. Also added code for
% conditional page margins, depending on paper size. Thanks to Uwe Ziegenhagen
% for the suggestions.

% 2006-08
% Made changes based on suggestions from Gene Cooperman. <gene at ccs.neu.edu>


% To Do:
% \listoffigures \listoftables
% \setcounter{secnumdepth}{0}


% This sets page margins to .5 inch if using letter paper, and to 1cm
% if using A4 paper. (This probably isn't strictly necessary.)
% If using another size paper, use default 1cm margins.
\ifthenelse{\lengthtest { \paperwidth = 11in}}
        { \geometry{top=.5in,left=.5in,right=.5in,bottom=.5in} }
        {\ifthenelse{ \lengthtest{ \paperwidth = 297mm}}
                {\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
                {\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
        }

% Turn off header and footer
\pagestyle{empty}


% Redefine section commands to use less space
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%x
                                {\normalfont\large\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
                                {-1explus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%
                                {\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {1ex plus .2ex}%
                                {\normalfont\small\bfseries}}
\makeatother

% Define BibTeX command
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% Don't print section numbers
\setcounter{secnumdepth}{0}

\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt plus 0.5ex}

\DeclareMathOperator*{\argmax}{argmax}

% -----------------------------------------------------------------------

\begin{document}

\raggedright
\footnotesize
\begin{multicols}{3}

% multicol parameters
% These lengths are set only within the three main columns
%\setlength{\columnseprule}{0.25pt}
\setlength{\premulticols}{1pt}
\setlength{\postmulticols}{1pt}
\setlength{\multicolsep}{1pt}
\setlength{\columnsep}{2pt}

\begin{center}
     \Large{Reinforcement Learning Cheat Sheet} \\
\end{center}

\section{Agent-Environment Interface}
\includegraphics[width=\linewidth]{./images/Agent-Environment.png}
The Agent at each step $t$ receives a representation of the environment's \emph{state}, $\mathcal{S}_t \in \mathcal{S}$ and it selects an action $\mathcal{A}_t\in \mathcal{A}$. Then, as a consequence of its action the agent receives a \emph{reward}, $\mathcal{R}_{t + 1} \in \mathcal{R} \in \mathbb{R}$.

\section{Reward}
The total expected cumulative \emph{reward} is expressed as:
\begin{equation}
\mathcal{G}_t = \sum_{k = 0}^\mathcal{H} \gamma^kr_{t + k + 1}
\label{eq: total_reward}
\end{equation}
Where $\gamma$ is the \emph{discount factor} ($\gamma \in [0,1]$) and $\mathcal{H}$ is the \emph{horizon}, that can be infinite.

\subsection{Reward Hypothesis}
All goals can be described by the maximisation of expected cumulative \emph{rewards}.

\section{Policy}
A \emph{policy} $\pi$ is the behaviour function mapping a state to an action $\pi(s|a)$

\section{History}
The \emph{history} is defined as what the agent has seen until time-step $t$
\begin{equation}
\mathcal{H}_t = \mathcal{S}_1, \mathcal{A}_1, \mathcal{R}_1, \cdots, \mathcal{S}_t, \mathcal{A}_t, \mathcal{R}_t
\label{eq: history}
\end{equation}

\section{Model}
A \emph{model} is the representation of the environment and predicts what the environment will do next (model $\neq$ environment)
\subsection{Transition model}
A \emph{transition model} predicts the next state 
\begin{equation}
\mathcal{P}_{ss'}^a = \mathbb{P}[\mathcal{S}'=s'|\mathcal{S}=s, \mathcal{A}=a]
\label{eq: transition_model}
\end{equation}
\subsection{Reward model}
A \emph{reward model} predicts the next immediate rewards 
\begin{equation}
\mathcal{R}_{ss'}^a = \mathbb{E}[\mathcal{R}|\mathcal{S}=s, \mathcal{A}=a]
\label{eq: reward_model}
\end{equation}

\newlength{\MyLen}
\settowidth{\MyLen}{\texttt{letterpaper}/\texttt{a4paper} \ }

\section{Value Function}
The \emph{value function} informs the agent how good is a state based on the expected cumulative reward following policy $\pi$.
\begin{equation}
v_{\pi}(s) = \mathbb{E}_{\pi}[\mathcal{G}_t | \mathcal{S}_t=s]
\label{eq: value_function}
\end{equation}

In some state $s$ and time-step $t$, the value function informs the agent of the expected sum of future rewards on a given policy $\pi$, so as to choose the right action from that state, that maximises that expected sum of rewards.

\section{Markov Decision Process}

\subsection{Markov State}
A state is \emph{Markov} if and only if 
\begin{equation}
\mathbb{P}[\mathcal{S}_{t+1}|\mathcal{S}_t] = \mathbb{P}[\mathcal{S}_{t+1}|\mathcal{S}_1, \cdots, \mathcal{S}_t]
\label{eq: markov_state}
\end{equation}

A \textbf{Markov Decision Process} (MPD), is a tuple $(\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma)$ where:
\begin{equation}
        \begin{array}{l}
         	\mathcal{S}\text{ is a finite set of Markov states:} \\
         	\mathcal{A}\text{ is a finite set of actions:} \\
         	\mathcal{P}\text{ is a state transition probability matrix:} \\
        	\mathcal{P}_{ss'}^a = \mathbb{P} \{\mathcal{S}_{t + 1} = s' | \mathcal{S}_t = s, \mathcal{A}_t = a \} \\
		   	\mathcal{R}\text{ is the expected reward:}\\
        	\mathcal{R}_{ss'}^a = \mathbb{E}[\mathcal{R}_{t + 1} | \mathcal{S}_{t + 1} = s',  \mathcal{S}_t= s, \mathcal{A}_t = a ]  \\
        \end{array}
\end{equation}

\section{State-Value Function - v}
The \emph{state-value} function of an MDP is the expected return starting from state $s$, following policy $\pi$:
\begin{equation}
v_{\pi}(s) = \mathbb{E}_{\pi}[\mathcal{G}_t | \mathcal{S}_t = s]
\label{eq: value_func}
\end{equation}

\section{Action-Value Function - q}
The \emph{action-value} function of an MDP is the expected return starting from state $s$, taking action $a$, and following policy $\pi$:
\begin{equation}
q_{\pi}(s, a) = \mathbb{E}_{\pi}[\mathcal{G}_t | \mathcal{S}_t = s, \mathcal{A}_t = a]
\label{eq: q_func}
\end{equation}

\section{Bellman Expectation Equation}
\begin{equation}
	\begin{array}{l}
		v_{\pi}(s) = \sum\limits_{a \in \mathcal{A}} \pi(a|s) \Big ( \mathcal{R}_s^a + \gamma \sum\limits_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^a v_{\pi}(s') \Big )  \\
		q_{\pi}(s, a) = \mathcal{R}_s^a + \gamma \sum\limits_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^a \sum\limits_{a' \in \mathcal{A}} \pi(s'|a') q_{\pi}(s',a')
	\end{array}
\end{equation}

\subsection{Equivalence $v_{\pi}$ - $q_{\pi}$}
\begin{equation}
	\begin{array}{l}
		v_{\pi}(s) = \sum\limits_{a \in \mathcal{A}} \pi(s|a) q_{\pi}(s,a) \\
		q_{\pi}(s,a) = \mathcal{R}_s^a + \gamma \sum\limits_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^a v_{\pi}(s')
	\end{array}
\end{equation}

\section{Optimality}
\subsection{Theorem of Optimality}
A policy $\pi(s|a)$ achieves the optimal value from state $s$, $v_{\pi}(s)=v_*(s)$ if and only if for any state $s'$ reachable from $s$, $\pi$ achieves the optimal value from state $s'$, $v_{\pi}(s') = v_*(s')$.

\begin{equation}
	\pi_*(s|a) = \begin{cases}
						1 \text{, if } a = \argmax\limits_{a \in \mathcal{A}} q_*(s,a) \\
						0
					\end{cases}
\end{equation}

\subsection{Optimal Value Function}
\begin{equation}
	\begin{array}{l}
		v_*(s) = \max\limits_{\pi}v_{\pi}(s) \\
		q_*(s,a) = \max\limits_{\pi}q_{\pi}(s)
	\end{array}
\end{equation}

\subsection{Bellman Optimality Equation}
\begin{equation}
	\begin{array}{l}
		v_*(s) = \max\limits_{a} \Big ( \mathcal{R}_s^a + \gamma \sum\limits_{s' \in S} \mathcal{P}_{ss'}^a v_*(s') \Big ) \\
		q_*(s,a) = \mathcal{R}_s^a + \gamma \sum\limits_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^a \max\limits_{a'} q_*(s',a')
	\end{array}
\end{equation}

\section{Contraction Mapping}

\subsection{Definition}

Let $(X, d)$ be a metric space and $f: X \rightarrow X$. We say that $f$ is a
\emph{contraction} if there is a real number $k \in [0, 1)$ such that
\begin{equation*}
    d(f(x), f(y)) \leq k d(x, y)
\end{equation*}

for all $x$ and $y$ in $X$, where the term $k$ is called a \emph{Lipschitz coefficent} for $f$.

\subsection{Contraction Mapping theorem}

Let $(X,d)$ be a complete metric space and let $f: X \rightarrow X$ be a contraction.
Then there is one and only one fixed point $x^\ast$ such that
\begin{equation*}
 f(x^\ast) = x^\ast
\end{equation*}

Moreover, if $x$ is any point in $X$ and $f^n(x)$ is inductively defined by
$f^2(x) = f(f(x))$, $f^3(x)=f(f^2(x))$, \ldots, $f^n(x)=f(f^{nâˆ’1}(x))$,
then $f^n(x) \rightarrow x^\ast$ as $n \rightarrow \infty$. This theorem guarantees
a unique optimal solution for the dynamic programming algorithms detailed below.

\section{Dynamic Programming}
Taking advantages of the subproblem structure of the V and Q function we can find the optimal policy by just \emph{planning}
\subsection{Policy Iteration}
We can now find the optimal policy
\begin{algorithm}[H]
%\SetAlgoLined
 1. Initialisation \\
 $v(s) \in \mathbb{R}, (\text{e.g. } V(s) = 0)$ and $\pi(s) \in \mathcal{A}$  for all $s \in \mathcal{S}$,\\
 $\Delta \leftarrow 0$ \\
 2. Policy Evaluation \\
 \While{$\Delta \ge \theta$ (a small positive number)}{
  \ForEach{$s \in \mathcal{S}$} {
        $v \leftarrow v(s)$\\
        $v(s) \leftarrow \sum\limits_a \pi(a|s) \sum\limits_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^a [\mathcal{R}_s^a + \gamma v(s')]$ \\
        $\Delta \leftarrow \max(\Delta, | v - v(s) |)$
        }
 }
 
 3. Policy Improvement \\
 \emph{policy-stable} $ \leftarrow $ \emph{true} \\
  \ForEach{$s \in \mathcal{S}$} {
        \emph{old-action} $\leftarrow \pi(s)$
        $\pi(s) \leftarrow \argmax\limits_a\sum\limits_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^a [r + \gamma v(s')]$ \\
        \emph{policy-stable} $\leftarrow$ \emph{old-action} $ = \pi(s)$
  }
if \emph{policy-stable} return $v$ $\approx v_*$ and $\pi \approx \pi_*$, else go to 2.
\caption{Policy Iteration}
\end{algorithm}

\end{multicols}

\begin{multicols}{3}

\subsection{Value Iteration}
We can avoid to wait until $v(s)$ has converged and instead do policy improvement and truncated policy evaluation step in one operation
\begin{algorithm}[H]
  \SetKwInOut{Output}{ouput}
 Initialise $v(s) \in \mathbb{R}, $ e.g $v(s) = 0$ \\
 $\Delta \leftarrow 0$ \\
 \While{$\Delta \ge \theta$ (a small positive number)}{
  \ForEach{$s \in \mathcal{S}$} {
        $v \leftarrow v(s)$ \\
        $v(s) \leftarrow \max\limits_a \sum\limits_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^a[\mathcal{R}_s^a + \gamma v(s')]$ \\
        $\Delta \leftarrow \max(\Delta, | v - v(s)|)$
  }
 }
 \Output{Deterministic policy $\pi \approx \pi_*$ such that}
 $\pi(s) = \argmax\limits_a \sum\limits_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^a [\mathcal{R}_s^a + \gamma v(s')]$
\caption{Value Iteration}
\end{algorithm}

\section{Monte-Carlo Methods}
Monte-Carlo (MC) are \emph{Model-Free} methods, that do not require complete knowledge of the environment but learn from \textbf{complete} episodes. It is based on \textbf{averaging sample returns} for each state-action pair.

\begin{algorithm}[H]
 Initialise for all $s \in \mathcal{S}, a \in \mathcal{A}:$ \\
        $\quad q(s,a) \leftarrow \text{arbitrary}$ \\
        $\quad \pi(s) \leftarrow \text{arbitrary}$ \\
        $\quad Returns(s,a) \leftarrow \text{empty list}$ \\

 \While{forever}{
    Choose $\mathcal{S}_0 \in \mathcal{S}$ and $\mathcal{A}_0 \in \mathcal{A}$, all pairs have probability $ > 0$ \\
    Generate an episode starting at $\mathcal{S}_0, \mathcal{A}_0$ following $\pi$
    \ForEach{pair $s,a$ appearing in the episode}
    {
        $\mathcal{G} \leftarrow$ return following the first occurrence of $s,a$ \\
        Append $\mathcal{G}$ to $Returns(s,a))$ \\
        $q(s,a) \leftarrow average(Returns(s,a))$ \\
    }
    \ForEach{$s$ in the episode}
    {
        $\pi(s) \leftarrow \argmax\limits_a q(s,a)$
    }
 }
\caption{Monte Carlo first-visit }
\end{algorithm}

For non-stationary problems, the Monte Carlo estimate for, e.g, $v$ is:
\begin{equation}
V(S_t) \leftarrow V(S_t) + \alpha \begin{bmatrix}
        G_t - V(S_t)
\end{bmatrix}
\end{equation}
Where $\alpha$ is the learning rate, how much we want to forget about past experiences.

\section{Sarsa}

Sarsa (State-action-reward-state-action) is a on-policy TD control. The update rule:

\begin{equation*}
    Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[r_t + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \right]
\end{equation*}

\subsection{$n$-step Sarsa}

Define the $n$-step Q-Return

\begin{equation*}
    q^{(n)} = R_{t+1} + \gamma R{t+2} + \ldots + \gamma^{n-1} R_{t+n} + \gamma^n Q(S_{t+n})
\end{equation*}

$n$-step Sarsa update $Q(S, a)$ towards the $n$-step Q-return

\begin{equation*}
    Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[q_t^{(n)} - Q(s_t, a_t) \right]
\end{equation*}

\subsection{Forward View Sarsa($\lambda$)}

\begin{equation*}
    q_t^\lambda = (1-\lambda) \sum_{n=1}^\infty \lambda^{n-1} q_t^{(n)}
\end{equation*}

Forward-view Sarsa($\lambda$):

\begin{equation*}
    Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[q_t^\lambda - Q(s_t, a_t) \right]
\end{equation*}

\begin{algorithm}[H]
 Initialise $Q(s,a)$ arbitrarily and $Q(terminal-state, Â·) = 0$\\
  \ForEach{episode $\in$ episodes}{
        Choose $a$ from $s$ using policy derived from $Q$ (e.g., $\epsilon$-greedy) \\
        \While{$s$ is not terminal} {
        Take action $a$, observer $r, s'$ \\
        Choose $a'$ from $s'$ using policy derived from $Q$ (e.g., $\epsilon$-greedy) \\
                $Q(s,a) \leftarrow Q(s,a) + \alpha  \begin{bmatrix}
r +  \gamma Q(s',a') - Q(s,a)
\end{bmatrix}$\\
        $s \leftarrow s'$ \\
        $a \leftarrow a'$
        }
 }
    \caption{Sarsa($\lambda$)}
\end{algorithm}

\section{Temporal Difference - Q Learning}
Temporal Difference (TD) methods learn directly from raw experience without a model of the environment's dynamics. TD substitutes the expected discounted reward $G_t$ from the episode with an estimation:
\begin{equation}
V(S_t) \leftarrow V(S_t) + \alpha \begin{bmatrix}
 R_{t + 1} + \gamma V(S_{t+1} - V(S_t)
\end{bmatrix}
\end{equation}
The following algorithm gives a generic implementation.
\begin{algorithm}[H]
 Initialise $Q(s,a)$ arbitrarily and $Q(terminal-state, Â·) = 0$\\
  \ForEach{episode $\in$ episodes}{
        \While{$s$ is not terminal} {
        Choose $a$ from $s$ using policy derived from $Q$ (e.g., $\epsilon$-greedy) \\
        Take action $a$, observer $r, s'$ \\
        $Q(s,a) \leftarrow Q(s,a) + \alpha  \begin{bmatrix}
r +  \gamma \max\limits_{a'}Q(s',a') - Q(s,a)
\end{bmatrix}$\\
        $s \leftarrow s'$
        }
 }
\caption{Q Learning}
\end{algorithm}


\section{Deep Q Learning}
Created by $DeepMind$, Deep Q Learning, DQL, substitutes the $Q$ function with a deep neural network called \emph{Q-network}. It also keep track of some observation in a $memory$ in order to use them to train the network.
\begin{equation}
L_i(\theta_i) = \mathbb{E}_{(s, a, r, s') \sim U(D)}\begin{bmatrix}
        ( \underbrace{r + \gamma \max\limits_a Q(s',a'; \theta_{i-1})}_\text{target} - \underbrace{Q(s,a;\theta_i)}_\text{prediction})^2
\end{bmatrix}
\end{equation}
Where $\theta$ are the weights of the network and $U(D)$ is the experience replay history.

\begin{algorithm}[H]
 Initialise replay memory $D$ with capacity $N$\\
 Initialise $Q(s,a)$ arbitrarily \\
 \ForEach{episode $\in$ episodes}{
        \While{$s$ is not terminal} {
         With probability $\epsilon$ select a random action $a \in A(s)$ \\
         otherwise select $a = \max_a Q(s, a; \theta)$ \\
         Take action $a$, observer $r, s'$ \\
         Store transition $(s, a, r, s')$ in $D$ \\
         Sample random minibatch of transitions $(s_j, a_j, r_j, s'_j)$ from $D$ \\
         $\text{Set } y_j \leftarrow
           \begin{cases}
               r_j & \text{for terminal } s_j'\\
               r_j + \gamma \max\limits_a Q(s',a'; \theta) & \text{for non-terminal } s'_j
           \end{cases}$ \\
         Perform gradient descent step on $(y_j - Q(s_j, a_j;\Theta))^2 $ \\
         $s \leftarrow s'$
        }
 }
\caption{Deep Q Learning}
\end{algorithm}

%---------------------------------------------------------------------------

\rule{0.3\linewidth}{0.25pt} \\
Copyright \copyright\ 2018 Francesco Saverio Zuppichini

\scriptsize

\href{https://github.com/FrancescoSaverioZuppichini/Reinforcement-Learning-Cheat-Sheet}{https://github.com/FrancescoSaverioZuppichini/Reinforcement-Learning-Cheat-Sheet}

\end{multicols}

\end{document}